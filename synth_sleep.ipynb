{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries \n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature):\n",
    "        super().__init__()\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction = 'sum') #reduction sum means output will be summed\n",
    "        self.similiratiy = nn.CosineSimilarity(dim=1) #cossine similarity\n",
    "        self.temperature = temperature #scaling factor\n",
    "\n",
    "        @staticmethod # doesn't depent upon the instance of the class means doesn't require self or cls\n",
    "        def mask_correlated_samples(batch_size):\n",
    "            n = 2*batch_size # each samples have two augmentations in contrastive learning (original + augmented)\n",
    "            mask = torch.ones((n, n), dtype= bool) # created a nXn matrix with all true values\n",
    "            mask = mask.fill_diagonal_(0) # filled diagonal elements with zero = False; so that samples shoudn't be campare with it self\n",
    "\n",
    "            # masking the elements\n",
    "            for i in range(batch_size): # ensuring that correlated pairs are excluded from neagative samples\n",
    "                mask[i, batch_size+i] = 0\n",
    "                mask[batch_size+i, i] = 0\n",
    "            return mask\n",
    "        \n",
    "        def forward(self, z_i, z_j):\n",
    "            batch_size = z_j.shape[0]\n",
    "            n = 2*batch_size\n",
    "            z = torch.cat((z_i, z_j), dim = 1) # number of rows remains same but columns increased\n",
    "            z = F.normalize(z, dim=1) # rows wise noramlization // it's common in contrastive learning\n",
    "\n",
    "            mask = self.mask_correlated_samples(batch_size)\n",
    "            simi_larity = self.similiratiy(z.unsqueeze(1), z.unsqueeze(0))\n",
    "\n",
    "            simi_larity_i_j = torch.diag(simi_larity, batch_size)\n",
    "            simi_larity_j_i = torch.diag(simi_larity, batch_size)\n",
    "\n",
    "            # number of positive samples and negative samples\n",
    "            positive_samples = torch.cat((simi_larity_i_j, simi_larity_j_i), dim=0).reshape(n,1)\n",
    "            negative_samples = simi_larity[mask].reshape(n, -1) # n rows and self decided columns\n",
    "\n",
    "            labels = torch.from_numpy(np.array([0]*n)).reshape(-1).to(positive_samples.device).long() #flatten the output and sending it to the GPU converting it in the GPU supporting format; and zero represents correct class\n",
    "            logits = torch.cat((positive_samples, negative_samples), dim=1) / self.temperature #similarity score\n",
    "\n",
    "            loss = self.criterion(logits, labels)\n",
    "            loss /= n\n",
    "\n",
    "            return loss, (labels, logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True],\n",
       "        [ True, False,  True,  True],\n",
       "        [ True,  True, False,  True],\n",
       "        [ True,  True,  True, False]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((4, 4), dtype= bool).fill_diagonal_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "---\n",
    "### NTXent Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Modality Specific Backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
